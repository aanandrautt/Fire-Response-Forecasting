{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0f9dcc-5bb6-42ad-95b9-c9cfee68abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\PyTorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd            # Data handling\n",
    "import numpy as np             # Numerical operations\n",
    "import torch                   # PyTorch core\n",
    "import torch.nn as nn          # Neural network modules\n",
    "import random\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "from sklearn.model_selection import train_test_split  # Data splitting\n",
    "from sklearn.preprocessing import MinMaxScaler       # Scaling\n",
    "import matplotlib.pyplot as plt  # (Optional) plotting results\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import joblib\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a38332-ef5c-4ec6-8117-b1be0bb6ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68988f5-da70-49f8-9153-507d28e7b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load the CSV file ---\n",
    "#csv_file = 'deflection_with_temperatures_final.csv'  # your CSV file\n",
    "csv_file = 'dataset.csv'  # your CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.drop(columns=[\"Time\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3602ce1-90aa-4d71-89e4-fb42a26380bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_suffixes = [\"x2\", \"y2\", \"x6\", \"y6\", \"x13\", \"y13\", \"x29\", \"y29\", \"x31\", \"y31\", \"x33\", \"y33\", \"T3\", \"T8\", \"T12\", \"T14\", \"T16\", \"T21\", \"T24\", \"T26\", \"T27\"]  # 21 names\n",
    "output_feature_suffixes = ['x2', 'y2', 'y3', 'TS3', 'x6', 'y6',  'x7', 'y7', 'y8', 'TS8', 'x11', 'y11', 'y12', 'TS12', 'x13', 'y13', 'y14', 'TS14', 'y16', 'TS16', 'x17', \n",
    "                           'y17', 'x19', 'y19', 'y21', 'TS21', 'x22', 'y22', 'y24', 'TS24', 'x25', 'y25', 'y26', 'TS26',  'y27', 'TS27', 'x29', 'y29', 'x31', 'y31', 'x33', 'y33',\"T3\", \"T8\", \"T12\", \"T14\", \"T16\", \"T21\", \"T24\", \"T26\", \"T27\"]  # 42+9 names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996bc2a1-0b47-47b5-81b8-72c254a261dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 181, 21) (2400, 181, 51)\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Parse data case by case ---\n",
    "X_cases = []\n",
    "y_cases = []\n",
    "\n",
    "num_cases = 2400\n",
    "timesteps = 181\n",
    "\n",
    "for i in range(num_cases):\n",
    "    prefix = f'case{i+1}_'\n",
    "    \n",
    "    input_cols = [prefix + feat for feat in input_feature_suffixes]\n",
    "    output_cols = [prefix + feat for feat in output_feature_suffixes]\n",
    "\n",
    "    # --- Defensive check: ensure all columns exist ---\n",
    "    missing_inputs = [col for col in input_cols if col not in df.columns]\n",
    "    missing_outputs = [col for col in output_cols if col not in df.columns]\n",
    "    if missing_inputs or missing_outputs:\n",
    "        raise ValueError(f\"Missing columns in case {i+1}: {missing_inputs + missing_outputs}\")\n",
    "    \n",
    "    X_case = df[input_cols].values  # shape: (181, 21)\n",
    "    y_case = df[output_cols].values  # shape: (181, 51)\n",
    "\n",
    "    X_cases.append(X_case)\n",
    "    y_cases.append(y_case)\n",
    "\n",
    "# --- Step 4: Stack into NumPy arrays ---\n",
    "X_data = np.stack(X_cases)  # (2400, 181, 21)\n",
    "y_data = np.stack(y_cases)  # (2400, 181, 51)\n",
    "\n",
    "print(X_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52609edc-a563-4082-add2-b9c536966a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1560, 181, 21)\n",
      "X_val: (480, 181, 21)\n",
      "X_test: (360, 181, 21)\n",
      "y_train: (1560, 181, 51)\n",
      "y_val: (480, 181, 51)\n",
      "y_test: (360, 181, 51)\n"
     ]
    }
   ],
   "source": [
    "# Flatten along cases (keep time series)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.15, random_state=42)\n",
    "\n",
    "# Then split train+val into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.23529, random_state=42)  # 0.1765 of 85% = 15%\n",
    "\n",
    "# Final shapes:\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_val:\", y_val.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cbcc678-2856-42e5-b748-79b043efb306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 37.4391  ,  -5.35601 ,   5.14348 ,  -5.85926 ,   0.326028,\n",
       "        -5.83046 , -37.4451  ,  -5.35659 ,  -5.14957 ,  -5.85983 ,\n",
       "        -0.332374,  -5.83102 ,  25.      ,  20.      ,  25.      ,\n",
       "        20.      ,  20.      ,  20.      ,  25.      ,  20.      ,\n",
       "        20.      ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[44][180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7aef4ff-e6dc-4c14-ad8e-7adcaa31df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Flatten the training data for fitting the scaler ----\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[2])  # shape: (samples * timesteps, features) → (181 * num_cases, 21)\n",
    "y_train_2d = y_train.reshape(-1, y_train.shape[2])  # shape: (samples * timesteps, outputs) → (181 * num_cases, 51)\n",
    "\n",
    "# ---- Fit scalers on training data only ----\n",
    "X_scaler = MinMaxScaler()\n",
    "X_train_norm_2d = X_scaler.fit_transform(X_train_2d)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_norm_2d = y_scaler.fit_transform(y_train_2d)\n",
    "\n",
    "# ---- Apply the same scaler to val and test data ----\n",
    "X_val_norm_2d = X_scaler.transform(X_val.reshape(-1, X_val.shape[2]))\n",
    "X_test_norm_2d = X_scaler.transform(X_test.reshape(-1, X_test.shape[2]))\n",
    "\n",
    "y_val_norm_2d = y_scaler.transform(y_val.reshape(-1, y_val.shape[2]))\n",
    "y_test_norm_2d = y_scaler.transform(y_test.reshape(-1, y_test.shape[2]))\n",
    "\n",
    "# ---- Reshape back to original shape ----\n",
    "X_train_norm = X_train_norm_2d.reshape(X_train.shape)\n",
    "X_val_norm = X_val_norm_2d.reshape(X_val.shape)\n",
    "X_test_norm = X_test_norm_2d.reshape(X_test.shape)\n",
    "\n",
    "y_train_norm = y_train_norm_2d.reshape(y_train.shape)\n",
    "y_val_norm = y_val_norm_2d.reshape(y_val.shape)\n",
    "y_test_norm = y_test_norm_2d.reshape(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1cc906-9b8b-4607-a3c7-e2ac3ec57ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (1560, 181, 51)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c67bfd7-09eb-42bc-a852-261e998adf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 181, 21)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd31cc19-2641-4737-8b6d-e27e24063d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['y_scaler.save']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the scalers\n",
    "joblib.dump(X_scaler, 'X_scaler.save')\n",
    "joblib.dump(y_scaler, 'y_scaler.save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ff20ac-c58a-4d0e-b614-187cd0b5c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize X_test_norm (must be 2D for inverse_transform)\n",
    "#X_test_flat = X_test_norm.reshape(-1, X_test_norm.shape[-1])  # shape: (360*181, 21)\n",
    "\n",
    "# Inverse transform\n",
    "#X_test_denorm_flat = X_scaler.inverse_transform(X_test_flat)  # shape: (360*181, 21)\n",
    "\n",
    "# Reshape back to original 3D shape\n",
    "#X_test_denorm = X_test_denorm_flat.reshape(X_test_norm.shape) \n",
    "\n",
    "#X_test_denorm[359][180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcdd67bd-cf03-4a95-9009-eb5a74acce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sliding: (235560, 30, 21)\n",
      "y_train_sliding: (235560, 51)\n"
     ]
    }
   ],
   "source": [
    "def create_sliding_windows(X, y, window_size=30, target_size=1):\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    num_cases, num_steps, num_features = X.shape\n",
    "\n",
    "    for case in range(num_cases):\n",
    "        for t in range(num_steps - window_size - target_size + 1):\n",
    "            X_window = X[case, t : t + window_size, :]                # shape: (30, 21)\n",
    "            y_target = y[case, t + window_size : t + window_size + target_size, :]  # shape: (1, 51)\n",
    "            X_seq.append(X_window)\n",
    "            y_seq.append(y_target.squeeze(0))  # make shape (51,) instead of (1,51)\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Create sliding windows\n",
    "X_train_sliding, y_train_sliding = create_sliding_windows(X_train_norm, y_train_norm, 30, 1)\n",
    "X_val_sliding, y_val_sliding = create_sliding_windows(X_val_norm, y_val_norm, 30, 1)\n",
    "X_test_sliding, y_test_sliding = create_sliding_windows(X_test_norm, y_test_norm, 30, 1)\n",
    "\n",
    "# Print shapes\n",
    "print(\"X_train_sliding:\", X_train_sliding.shape)  # (N, 30, 21)\n",
    "print(\"y_train_sliding:\", y_train_sliding.shape)  # (N, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c98ed90-e4ee-4fbc-adb8-807772f06080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (N, window_size, in_dim)  e.g. (N, 30, 21)\n",
    "        Y: np.array of shape (N, out_dim)              e.g. (N, 51)\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # returns: (30,21), (51,)\n",
    "        return self.X[idx], self.Y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceb0428f-1cfe-4e91-b464-53e183e6a405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding-window loaders:\n",
      "  train batches: 7362\n",
      "  val batches: 2265\n",
      "  test batches: 1699\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  # or whatever you prefer\n",
    "\n",
    "# Train sliding dataset (optional: for your old teacher-forcing model / metrics)\n",
    "train_dataset_sliding = SlidingWindowDataset(X_train_sliding, y_train_sliding)\n",
    "\n",
    "# Validation and test sliding datasets\n",
    "val_dataset_sliding   = SlidingWindowDataset(X_val_sliding,   y_val_sliding)\n",
    "test_dataset_sliding  = SlidingWindowDataset(X_test_sliding,  y_test_sliding)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader_sliding = DataLoader(train_dataset_sliding, batch_size=batch_size, shuffle=True)\n",
    "val_loader_sliding   = DataLoader(val_dataset_sliding,   batch_size=batch_size, shuffle=False)\n",
    "test_loader_sliding  = DataLoader(test_dataset_sliding,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Sliding-window loaders:\")\n",
    "print(\"  train batches:\", len(train_loader_sliding))\n",
    "print(\"  val batches:\",   len(val_loader_sliding))\n",
    "print(\"  test batches:\",  len(test_loader_sliding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5c43049-9ce4-43b8-b840-53c5de180089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train full-sequence batches: 49\n"
     ]
    }
   ],
   "source": [
    "class FullSequenceDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (N_cases, T, in_dim)  e.g. (N, 181, 21)\n",
    "        Y: np.array of shape (N_cases, T, out_dim) e.g. (N, 181, 51)\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == Y.shape[0]\n",
    "        assert X.shape[1] == Y.shape[1]\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # returns full sequence for one case\n",
    "        return self.X[idx], self.Y[idx]   # shapes: (T, 21), (T, 51)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_full = FullSequenceDataset(X_train_norm, y_train_norm)\n",
    "val_dataset_full   = FullSequenceDataset(X_val_norm,   y_val_norm)   # optional, if needed\n",
    "test_dataset_full  = FullSequenceDataset(X_test_norm,  y_test_norm)  # optional\n",
    "\n",
    "# DataLoader for training with scheduled sampling\n",
    "batch_size = 64  # tune if needed\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Train full-sequence batches:\", len(train_loader_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71fa4585-f214-4cca-a44f-1895c006cb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_in_list: [0, 1, 4, 5, 14, 15, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
     ]
    }
   ],
   "source": [
    "# Build mapping from suffix -> index in the 51-dim output\n",
    "suffix_to_out_idx = {suf: i for i, suf in enumerate(output_feature_suffixes)}\n",
    "\n",
    "# Get indices for the 21 input features in the same order\n",
    "idx_in_list = [suffix_to_out_idx[suf] for suf in input_feature_suffixes]\n",
    "print(\"idx_in_list:\", idx_in_list)\n",
    "\n",
    "# Convert to tensor (later moved to device)\n",
    "idx_in = torch.tensor(idx_in_list, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6154f622-d84c-4211-8922-22791c9f9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_scheduled(\n",
    "    model,\n",
    "    params,\n",
    "    train_loader_full,\n",
    "    val_loader_sliding,\n",
    "    device,\n",
    "    idx_in,\n",
    "    max_epochs=2000,\n",
    "    window_size=30,\n",
    "    rollout_horizon=8,\n",
    "):\n",
    "    # ---- Loss function ----\n",
    "    if params['loss_function'] == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif params['loss_function'] == 'huber':\n",
    "        criterion = nn.HuberLoss(delta=params.get('huber_delta', 1.0))\n",
    "    elif params['loss_function'] == 'mae':\n",
    "        criterion = nn.L1Loss()\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "    \n",
    "    # ---- Optimizer ----\n",
    "    if params['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "    elif params['optimizer'] == 'adamw':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "    elif params['optimizer'] == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params['weight_decay']\n",
    "        )\n",
    "    \n",
    "    # ---- Scheduler ----\n",
    "    if params['scheduler'] == 'plateau':\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=10\n",
    "        )\n",
    "    elif params['scheduler'] == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=max_epochs, eta_min=1e-6\n",
    "        )\n",
    "    elif params['scheduler'] == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=30, gamma=0.5\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "    \n",
    "    grad_clip = params.get('grad_clip', None)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    patience = 2000\n",
    "    patience_counter = 0\n",
    "\n",
    "    # ============================\n",
    "    # NEW: loss history containers\n",
    "    # ============================\n",
    "    train_losses = []\n",
    "    val_losses   = []\n",
    "    \n",
    "    # move idx_in to device once\n",
    "    idx_in = idx_in.to(device)\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # ---- scheduled sampling probability ----\n",
    "        import math\n",
    "        center = max_epochs / 2\n",
    "        sigma  = max_epochs / 6\n",
    "        max_pred_prob = 0.7\n",
    "\n",
    "        x = epoch\n",
    "        use_pred_prob = max_pred_prob * math.exp(\n",
    "            - ((x - center) ** 2) / (2 * sigma ** 2)\n",
    "        )\n",
    "        \n",
    "        for X_seq, Y_seq in train_loader_full:\n",
    "            X_seq = X_seq.to(device)\n",
    "            Y_seq = Y_seq.to(device)\n",
    "            B, T, _ = X_seq.shape\n",
    "            \n",
    "            w = window_size\n",
    "            h = rollout_horizon\n",
    "            \n",
    "            max_start = T - w - h\n",
    "            if max_start <= 0:\n",
    "                raise ValueError(\"Sequence too short for given window_size and rollout_horizon.\")\n",
    "            \n",
    "            s = random.randint(0, max_start)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0.0\n",
    "            \n",
    "            window_in = X_seq[:, s:s+w, :]\n",
    "            \n",
    "            for step in range(h):\n",
    "                y_pred = model(window_in)\n",
    "                y_true = Y_seq[:, s + w + step, :]\n",
    "                \n",
    "                batch_loss = batch_loss + criterion(y_pred, y_true)\n",
    "                \n",
    "                x_true_next = y_true[:, idx_in]\n",
    "                x_pred_next = y_pred[:, idx_in]\n",
    "                \n",
    "                if use_pred_prob <= 0.0:\n",
    "                    x_next = x_true_next\n",
    "                elif use_pred_prob >= 1.0:\n",
    "                    x_next = x_pred_next\n",
    "                else:\n",
    "                    mask = (torch.rand(B, 1, device=device) < use_pred_prob).float()\n",
    "                    x_next = mask * x_pred_next + (1.0 - mask) * x_true_next\n",
    "                \n",
    "                x_next = x_next.unsqueeze(1)\n",
    "                window_in = torch.cat([window_in, x_next], dim=1)[:, -w:, :]\n",
    "            \n",
    "            batch_loss = batch_loss / h\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += batch_loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader_full)\n",
    "        \n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader_sliding:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader_sliding)\n",
    "\n",
    "        # ============================\n",
    "        # NEW: save losses per epoch\n",
    "        # ============================\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Scheduler step\n",
    "        if scheduler:\n",
    "            if params['scheduler'] == 'plateau':\n",
    "                scheduler.step(avg_val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch [{epoch}/{max_epochs}] \"\n",
    "            f\"Train Loss: {avg_train_loss:.6f} \"\n",
    "            f\"Val Loss: {avg_val_loss:.6f} \"\n",
    "            f\"use_pred_prob={use_pred_prob:.3f}\"\n",
    "        )\n",
    "        \n",
    "        # ---- Early stopping ----\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    # ---- load best weights ----\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # ============================\n",
    "    # NEW: save model & losses\n",
    "    # ============================\n",
    "    torch.save(\n",
    "        {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"params\": params,\n",
    "        },\n",
    "        \"training_losses.pth\"\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"params\": params,\n",
    "        },\n",
    "        \"final_trained_model.pth\"\n",
    "    )\n",
    "    \n",
    "    return best_val_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8b77b9-dc63-451c-ae49-961f998895e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=21, hidden_size=128, num_layers=3, \n",
    "                 output_size=51, dropout_p=0.2, activation='relu'):\n",
    "        super(OptimizedLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        \n",
    "        # First LSTM layer\n",
    "        self.lstm_layers.append(nn.LSTM(input_size, hidden_size, batch_first=True))\n",
    "        \n",
    "        # Additional LSTM layers\n",
    "        for i in range(num_layers - 1):\n",
    "            self.lstm_layers.append(nn.LSTM(hidden_size, hidden_size, batch_first=True))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Pass through LSTM layers\n",
    "        for i, lstm in enumerate(self.lstm_layers):\n",
    "            out, _ = lstm(x)\n",
    "            if i < len(self.lstm_layers) - 1:  # Don't apply dropout after last LSTM\n",
    "                out = self.dropout(self.activation(out))\n",
    "            x = out\n",
    "        \n",
    "        # Take the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.activation(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09603a5d-9c13-4efd-9815-0228cb0fcd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest \n",
    "best_params = {\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.118,\n",
    "    'activation': 'leaky_relu',\n",
    "    'learning_rate':0.002616,\n",
    "    'weight_decay': 0.000026,\n",
    "    'optimizer': 'adamw',\n",
    "    'grad_clip': 1.8070018004980979,\n",
    "    'loss_function': 'huber',\n",
    "    'scheduler': 'cosine',\n",
    "    'huber_delta': 1.31\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba63bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build idx_in from suffix lists\n",
    "suffix_to_out_idx = {suf: i for i, suf in enumerate(output_feature_suffixes)}\n",
    "idx_in_list = [suffix_to_out_idx[suf] for suf in input_feature_suffixes]\n",
    "idx_in = torch.tensor(idx_in_list, dtype=torch.long)\n",
    "\n",
    "# Create model with best_params\n",
    "final_model = OptimizedLSTM(\n",
    "    input_size=21,\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    output_size=51,\n",
    "    dropout_p=best_params['dropout'],\n",
    "    activation=best_params['activation']\n",
    ").to(device)\n",
    "\n",
    "print(\"Training final model with scheduled sampling...\")\n",
    "final_val_loss, trained_model = train_model_scheduled(\n",
    "    model=final_model,\n",
    "    params=best_params,\n",
    "    train_loader_full=train_loader_full,\n",
    "    val_loader_sliding=val_loader_sliding,   # from your sliding-window step\n",
    "    device=device,\n",
    "    idx_in=idx_in,\n",
    "    max_epochs=800,\n",
    "    window_size=30,\n",
    "    rollout_horizon=8\n",
    ")\n",
    "\n",
    "print(\"Best validation loss (scheduled sampling):\", final_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7653d483-3d9c-41a3-91e4-eef209a81b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved loss history\n",
    "checkpoint = torch.load(\"training_losses.pth\", map_location=\"cpu\")\n",
    "\n",
    "train_losses = checkpoint[\"train_losses\"]\n",
    "val_losses   = checkpoint[\"val_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff032f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(train_losses, label=\"Training Loss\", linewidth=2)\n",
    "plt.plot(val_losses, label=\"Validation Loss\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "227b1031-34e7-47b1-a52e-3b927035bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_step_return_arrays(model, data_loader, device, loss_function='huber', huber_delta=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "    if loss_function == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_function == 'huber':\n",
    "        criterion = nn.HuberLoss(delta=huber_delta)\n",
    "    elif loss_function == 'mae':\n",
    "        criterion = nn.L1Loss()\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_preds.append(y_pred.cpu().numpy())\n",
    "            all_trues.append(y_batch.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)   # (N,51)\n",
    "    all_trues = np.concatenate(all_trues, axis=0)   # (N,51)\n",
    "\n",
    "    mae = mean_absolute_error(all_trues, all_preds)\n",
    "    mse = mean_squared_error(all_trues, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_trues, all_preds)\n",
    "\n",
    "    print(\"\\n=== One-step EVAL (teacher-forcing) ===\")\n",
    "    print(f\"Loss ({loss_function}): {avg_loss:.6f}\")\n",
    "    print(f\"MAE:   {mae:.6f}\")\n",
    "    print(f\"MSE:   {mse:.6f}\")\n",
    "    print(f\"RMSE:  {rmse:.6f}\")\n",
    "    print(f\"R²:    {r2:.4f}\")\n",
    "    \n",
    "    return avg_loss, mae, rmse, r2, all_trues, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe52f5b-a1d9-4668-aa27-f34c3550c111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== One-step EVAL (teacher-forcing) ===\n",
      "Loss (huber): 0.000128\n",
      "MAE:   0.006947\n",
      "MSE:   0.000256\n",
      "RMSE:  0.015997\n",
      "R²:    0.9854\n"
     ]
    }
   ],
   "source": [
    "avg_loss, mae, rmse, r2, y_true_norm, y_pred_norm = evaluate_one_step_return_arrays(\n",
    "    trained_model,\n",
    "    test_loader_sliding,\n",
    "    device,\n",
    "    loss_function=best_params.get('loss_function', 'huber'),\n",
    "    huber_delta=best_params.get('huber_delta', 1.0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06016c53-86d3-4276-ac3e-548934844fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_feature_metrics(y_true, y_pred, feature_names=None):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: arrays of shape (N_samples, n_features)\n",
    "    feature_names: optional list of length n_features\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "    n_features = y_true.shape[1]\n",
    "    rmses = []\n",
    "    r2s = []\n",
    "\n",
    "    for j in range(n_features):\n",
    "        mse_j = mean_squared_error(y_true[:, j], y_pred[:, j])\n",
    "        rmse_j = np.sqrt(mse_j)\n",
    "        r2_j = r2_score(y_true[:, j], y_pred[:, j])\n",
    "        rmses.append(rmse_j)\n",
    "        r2s.append(r2_j)\n",
    "\n",
    "    rmses = np.array(rmses)\n",
    "    r2s = np.array(r2s)\n",
    "\n",
    "    print(\"\\nPer-feature RMSE and R² (normalized space):\")\n",
    "    for j in range(n_features):\n",
    "        name = feature_names[j] if feature_names is not None else f\"f{j}\"\n",
    "        print(f\"{j:2d} ({name:8s}) → RMSE={rmses[j]:.4f}, R²={r2s[j]:.4f}\")\n",
    "\n",
    "    return rmses, r2s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90dd6251-66b4-4548-9cd0-7435737dd7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== One-step EVAL (teacher-forcing) ===\n",
      "Loss (huber): 0.000128\n",
      "MAE:   0.006947\n",
      "MSE:   0.000256\n",
      "RMSE:  0.015997\n",
      "R²:    0.9854\n",
      "\n",
      "Per-feature RMSE and R² (normalized space):\n",
      " 0 (x2      ) → RMSE=0.0076, R²=0.9935\n",
      " 1 (y2      ) → RMSE=0.0103, R²=0.9943\n",
      " 2 (y3      ) → RMSE=0.0231, R²=0.9476\n",
      " 3 (TS3     ) → RMSE=0.0270, R²=0.9897\n",
      " 4 (x6      ) → RMSE=0.0065, R²=0.9931\n",
      " 5 (y6      ) → RMSE=0.0098, R²=0.9963\n",
      " 6 (x7      ) → RMSE=0.0059, R²=0.9888\n",
      " 7 (y7      ) → RMSE=0.0149, R²=0.9702\n",
      " 8 (y8      ) → RMSE=0.0243, R²=0.9564\n",
      " 9 (TS8     ) → RMSE=0.0103, R²=0.9987\n",
      "10 (x11     ) → RMSE=0.0051, R²=0.9903\n",
      "11 (y11     ) → RMSE=0.0131, R²=0.9764\n",
      "12 (y12     ) → RMSE=0.0304, R²=0.9288\n",
      "13 (TS12    ) → RMSE=0.0110, R²=0.9987\n",
      "14 (x13     ) → RMSE=0.0057, R²=0.9925\n",
      "15 (y13     ) → RMSE=0.0095, R²=0.9958\n",
      "16 (y14     ) → RMSE=0.0241, R²=0.9461\n",
      "17 (TS14    ) → RMSE=0.0088, R²=0.9983\n",
      "18 (y16     ) → RMSE=0.0264, R²=0.9654\n",
      "19 (TS16    ) → RMSE=0.0110, R²=0.9989\n",
      "20 (x17     ) → RMSE=0.0060, R²=0.9892\n",
      "21 (y17     ) → RMSE=0.0124, R²=0.9826\n",
      "22 (x19     ) → RMSE=0.0057, R²=0.9959\n",
      "23 (y19     ) → RMSE=0.0165, R²=0.9733\n",
      "24 (y21     ) → RMSE=0.0289, R²=0.9168\n",
      "25 (TS21    ) → RMSE=0.0108, R²=0.9985\n",
      "26 (x22     ) → RMSE=0.0051, R²=0.9951\n",
      "27 (y22     ) → RMSE=0.0143, R²=0.9790\n",
      "28 (y24     ) → RMSE=0.0211, R²=0.9630\n",
      "29 (TS24    ) → RMSE=0.0118, R²=0.9987\n",
      "30 (x25     ) → RMSE=0.0064, R²=0.9902\n",
      "31 (y25     ) → RMSE=0.0135, R²=0.9841\n",
      "32 (y26     ) → RMSE=0.0271, R²=0.9695\n",
      "33 (TS26    ) → RMSE=0.0275, R²=0.9933\n",
      "34 (y27     ) → RMSE=0.0195, R²=0.9566\n",
      "35 (TS27    ) → RMSE=0.0109, R²=0.9985\n",
      "36 (x29     ) → RMSE=0.0071, R²=0.9975\n",
      "37 (y29     ) → RMSE=0.0124, R²=0.9928\n",
      "38 (x31     ) → RMSE=0.0062, R²=0.9971\n",
      "39 (y31     ) → RMSE=0.0112, R²=0.9962\n",
      "40 (x33     ) → RMSE=0.0072, R²=0.9951\n",
      "41 (y33     ) → RMSE=0.0107, R²=0.9965\n",
      "42 (T3      ) → RMSE=0.0147, R²=0.9976\n",
      "43 (T8      ) → RMSE=0.0149, R²=0.9979\n",
      "44 (T12     ) → RMSE=0.0189, R²=0.9970\n",
      "45 (T14     ) → RMSE=0.0143, R²=0.9970\n",
      "46 (T16     ) → RMSE=0.0164, R²=0.9979\n",
      "47 (T21     ) → RMSE=0.0157, R²=0.9976\n",
      "48 (T24     ) → RMSE=0.0192, R²=0.9972\n",
      "49 (T26     ) → RMSE=0.0204, R²=0.9970\n",
      "50 (T27     ) → RMSE=0.0194, R²=0.9968\n"
     ]
    }
   ],
   "source": [
    "avg_loss, mae, rmse, r2, y_true_norm, y_pred_norm = evaluate_one_step_return_arrays(\n",
    "    trained_model,\n",
    "    test_loader_sliding,\n",
    "    device,\n",
    "    loss_function=best_params.get('loss_function', 'huber'),\n",
    "    huber_delta=best_params.get('huber_delta', 1.0)\n",
    ")\n",
    "\n",
    "# If you know the physical meaning of each of the 51 outputs:\n",
    "feature_names = output_feature_suffixes  # length 51, from your earlier list\n",
    "\n",
    "rmses_feat, r2_feat = per_feature_metrics(\n",
    "    y_true_norm,\n",
    "    y_pred_norm,\n",
    "    feature_names=feature_names\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
